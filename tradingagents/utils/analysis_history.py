#!/usr/bin/env python3
"""
åˆ†æå†å²è®°å½•ç®¡ç†å™¨
è´Ÿè´£ä¿å­˜ã€æŸ¥è¯¢å’Œç®¡ç†è‚¡ç¥¨åˆ†æå†å²è®°å½•
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import uuid

@dataclass
class AnalysisRecord:
    """åˆ†æè®°å½•æ•°æ®ç±»"""
    record_id: str
    session_id: str
    stock_symbol: str
    analysis_date: str
    market_type: str
    analysts: List[str]
    research_depth: str
    llm_provider: str
    llm_model: str
    start_time: datetime
    end_time: datetime
    duration: float
    success: bool
    created_at: datetime
    results: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    token_usage: Optional[Dict[str, Any]] = None
    total_cost: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        data = asdict(self)
        # è½¬æ¢datetimeå¯¹è±¡ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
        data['start_time'] = self.start_time.isoformat()
        data['end_time'] = self.end_time.isoformat()
        data['created_at'] = self.created_at.isoformat()
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AnalysisRecord':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        # è½¬æ¢æ—¶é—´å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡
        data['start_time'] = datetime.fromisoformat(data['start_time'])
        data['end_time'] = datetime.fromisoformat(data['end_time'])
        data['created_at'] = datetime.fromisoformat(data['created_at'])
        return cls(**data)

class AnalysisHistoryManager:
    """åˆ†æå†å²è®°å½•ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.collection_name = "analysis_history"
        self._initialize_database()
    
    def _initialize_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        # åˆå§‹åŒ–é»˜è®¤å€¼
        self.collection = None
        self.mongodb_client = None
        self.mongodb_db = None
        self.redis_client = None
        self._use_file_storage = False
        
        try:
            from tradingagents.config.database_manager import get_database_manager
            db_manager = get_database_manager()
            
            if db_manager.mongodb_available:
                self.mongodb_client = db_manager.mongodb_client
                self.mongodb_db = self.mongodb_client[db_manager.mongodb_config["database"]]
                self.collection = self.mongodb_db[self.collection_name]
                
                # åˆ›å»ºç´¢å¼•
                self._create_indexes()
                self.logger.info("âœ… MongoDBå†å²è®°å½•ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            if db_manager.redis_available:
                self.redis_client = db_manager.redis_client
                self.logger.info("âœ… Rediså†å²è®°å½•ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶å­˜å‚¨: {e}")
            # ç¡®ä¿ä½¿ç”¨æ–‡ä»¶å­˜å‚¨
            self.collection = None
            self.mongodb_client = None
            self.mongodb_db = None
            self.redis_client = None
            self._use_file_storage = True
    
    def _create_indexes(self):
        """åˆ›å»ºMongoDBç´¢å¼•"""
        if self.collection is not None:
            try:
                # åˆ›å»ºå¤åˆç´¢å¼•
                self.collection.create_index([
                    ("stock_symbol", 1),
                    ("created_at", -1)
                ])
                # åˆ›å»ºå•å­—æ®µç´¢å¼•
                self.collection.create_index([("created_at", -1)])
                self.collection.create_index([("success", 1)])
                self.collection.create_index([("llm_provider", 1)])
                self.logger.info("âœ… å†å²è®°å½•ç´¢å¼•åˆ›å»ºå®Œæˆ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
    
    def save_analysis_record(self, 
                           session_id: str,
                           stock_symbol: str,
                           analysis_date: str,
                           market_type: str,
                           analysts: List[str],
                           research_depth: str,
                           llm_provider: str,
                           llm_model: str,
                           start_time: datetime,
                           end_time: datetime,
                           duration: float,
                           success: bool,
                           results: Optional[Dict[str, Any]] = None,
                           error_message: Optional[str] = None,
                           token_usage: Optional[Dict[str, Any]] = None,
                           total_cost: float = 0.0) -> str:
        """ä¿å­˜åˆ†æè®°å½•"""
        
        try:
            # åˆ›å»ºè®°å½•ID
            record_id = str(uuid.uuid4())
            
            # åˆ›å»ºè®°å½•å¯¹è±¡
            record = AnalysisRecord(
                record_id=record_id,
                session_id=session_id,
                stock_symbol=stock_symbol.upper(),
                analysis_date=analysis_date,
                market_type=market_type,
                analysts=analysts,
                research_depth=research_depth,
                llm_provider=llm_provider,
                llm_model=llm_model,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                success=success,
                created_at=datetime.now(),
                results=results,
                error_message=error_message,
                token_usage=token_usage,
                total_cost=total_cost
            )
            
            if hasattr(self, 'collection') and self.collection is not None:
                # ä¿å­˜åˆ°MongoDB
                self.collection.insert_one(record.to_dict())
                self.logger.info(f"âœ… åˆ†æè®°å½•å·²ä¿å­˜åˆ°MongoDB: {record_id}")
            else:
                # ä¿å­˜åˆ°æ–‡ä»¶
                self._save_to_file(record)
                self.logger.info(f"âœ… åˆ†æè®°å½•å·²ä¿å­˜åˆ°æ–‡ä»¶: {record_id}")
            
            # ç¼“å­˜åˆ°Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(self, 'redis_client') and self.redis_client is not None:
                self._cache_to_redis(record)
                
            return record_id
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜åˆ†æè®°å½•å¤±è´¥: {e}")
            # é™çº§åˆ°æ–‡ä»¶å­˜å‚¨
            try:
                self._save_to_file(record)
                return record_id
            except:
                return record_id
    
    def _save_to_file(self, record: AnalysisRecord):
        """ä¿å­˜è®°å½•åˆ°æ–‡ä»¶"""
        import os
        from pathlib import Path
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        history_dir = project_root / "data" / "analysis_history"
        history_dir.mkdir(parents=True, exist_ok=True)
        
        # æŒ‰æ—¥æœŸç»„ç»‡æ–‡ä»¶
        date_str = record.created_at.strftime("%Y-%m-%d")
        file_path = history_dir / f"analysis_{date_str}.jsonl"
        
        # è¿½åŠ è®°å½•åˆ°æ–‡ä»¶
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(record.to_dict(), ensure_ascii=False) + "\n")
    
    def _cache_to_redis(self, record: AnalysisRecord):
        """ç¼“å­˜è®°å½•åˆ°Redis"""
        try:
            # ç¼“å­˜æœ€è¿‘çš„è®°å½•ï¼Œè®¾ç½®è¿‡æœŸæ—¶é—´ä¸º7å¤©
            key = f"analysis_history:{record.record_id}"
            self.redis_client.setex(
                key, 
                int(timedelta(days=7).total_seconds()),
                json.dumps(record.to_dict(), ensure_ascii=False)
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ Redisç¼“å­˜å¤±è´¥: {e}")
    
    def get_analysis_history(self, 
                           stock_symbol: Optional[str] = None,
                           limit: int = 50,
                           offset: int = 0,
                           success_only: bool = False,
                           date_from: Optional[datetime] = None,
                           date_to: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """è·å–åˆ†æå†å²è®°å½•"""
        
        try:
            if hasattr(self, 'collection') and self.collection is not None:
                return self._get_from_mongodb(stock_symbol, limit, offset, success_only, date_from, date_to)
            else:
                return self._get_from_files(stock_symbol, limit, offset, success_only, date_from, date_to)
        except Exception as e:
            self.logger.error(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")
            return []
    
    def _get_from_mongodb(self, stock_symbol, limit, offset, success_only, date_from, date_to):
        """ä»MongoDBè·å–è®°å½•"""
        if self.collection is None:
            return []
            
        query = {}
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        if stock_symbol:
            query["stock_symbol"] = stock_symbol.upper()
        
        if success_only:
            query["success"] = True
        
        if date_from or date_to:
            date_query = {}
            if date_from:
                date_query["$gte"] = date_from
            if date_to:
                date_query["$lte"] = date_to
            if date_query:
                query["created_at"] = date_query
        
        # æ‰§è¡ŒæŸ¥è¯¢
        cursor = self.collection.find(query).sort("created_at", -1).skip(offset).limit(limit)
        records = list(cursor)
        
        # ç§»é™¤MongoDBçš„_idå­—æ®µ
        for record in records:
            if '_id' in record:
                del record['_id']
        
        return records
    
    def _get_from_files(self, stock_symbol, limit, offset, success_only, date_from, date_to):
        """ä»æ–‡ä»¶è·å–è®°å½•"""
        import os
        from pathlib import Path
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        history_dir = project_root / "data" / "analysis_history"
        
        if not history_dir.exists():
            return []
        
        records = []
        
        # è¯»å–æ‰€æœ‰å†å²æ–‡ä»¶ï¼ŒæŒ‰æ—¥æœŸå€’åº
        for file_path in sorted(history_dir.glob("analysis_*.jsonl"), reverse=True):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            record = json.loads(line.strip())
                            
                            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                            if stock_symbol and record.get("stock_symbol", "").upper() != stock_symbol.upper():
                                continue
                            
                            if success_only and not record.get("success", False):
                                continue
                            
                            # æ—¥æœŸè¿‡æ»¤
                            created_at = datetime.fromisoformat(record["created_at"])
                            if date_from and created_at < date_from:
                                continue
                            if date_to and created_at > date_to:
                                continue
                            
                            records.append(record)
                            
                            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°é™åˆ¶
                            if len(records) >= limit + offset:
                                break
                
                # å¦‚æœå·²è¾¾åˆ°é™åˆ¶ï¼Œåœæ­¢è¯»å–æ›´å¤šæ–‡ä»¶
                if len(records) >= limit + offset:
                    break
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ è¯»å–å†å²æ–‡ä»¶å¤±è´¥: {file_path}: {e}")
                continue
        
        # åº”ç”¨åç§»å’Œé™åˆ¶
        return records[offset:offset + limit]
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        try:
            if hasattr(self, 'collection') and self.collection is not None:
                return self._get_stats_from_mongodb()
            else:
                return self._get_stats_from_files()
        except Exception as e:
            self.logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def _get_stats_from_mongodb(self):
        """ä»MongoDBè·å–ç»Ÿè®¡ä¿¡æ¯"""
        if self.collection is None:
            return {}
            
        pipeline = [
            {
                "$group": {
                    "_id": None,
                    "total_analyses": {"$sum": 1},
                    "successful_analyses": {
                        "$sum": {"$cond": [{"$eq": ["$success", True]}, 1, 0]}
                    },
                    "total_cost": {"$sum": "$total_cost"},
                    "avg_duration": {"$avg": "$duration"},
                    "unique_stocks": {"$addToSet": "$stock_symbol"}
                }
            }
        ]
        
        result = list(self.collection.aggregate(pipeline))
        if result:
            stats = result[0]
            stats["unique_stocks_count"] = len(stats["unique_stocks"])
            stats.pop("unique_stocks")
            stats.pop("_id")
            return stats
        
        return {}
    
    def _get_stats_from_files(self):
        """ä»æ–‡ä»¶è·å–ç»Ÿè®¡ä¿¡æ¯"""
        import os
        from pathlib import Path
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        history_dir = project_root / "data" / "analysis_history"
        
        if not history_dir.exists():
            return {}
        
        total_analyses = 0
        successful_analyses = 0
        total_cost = 0.0
        total_duration = 0.0
        unique_stocks = set()
        
        for file_path in history_dir.glob("analysis_*.jsonl"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            record = json.loads(line.strip())
                            total_analyses += 1
                            
                            if record.get("success", False):
                                successful_analyses += 1
                            
                            total_cost += record.get("total_cost", 0.0)
                            total_duration += record.get("duration", 0.0)
                            unique_stocks.add(record.get("stock_symbol", ""))
                            
            except Exception as e:
                self.logger.warning(f"âš ï¸ è¯»å–ç»Ÿè®¡æ–‡ä»¶å¤±è´¥: {file_path}: {e}")
                continue
        
        return {
            "total_analyses": total_analyses,
            "successful_analyses": successful_analyses,
            "total_cost": total_cost,
            "avg_duration": total_duration / total_analyses if total_analyses > 0 else 0.0,
            "unique_stocks_count": len(unique_stocks)
        }
    
    def delete_old_records(self, days: int = 30):
        """åˆ é™¤æ—§çš„åˆ†æè®°å½•"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        try:
            if self.collection is not None:
                result = self.collection.delete_many({"created_at": {"$lt": cutoff_date}})
                self.logger.info(f"âœ… åˆ é™¤äº† {result.deleted_count} æ¡æ—§è®°å½•")
                return result.deleted_count
        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤æ—§è®°å½•å¤±è´¥: {e}")
            return 0

    def delete_record_by_id(self, record_id: str) -> bool:
        """æ ¹æ®è®°å½•IDåˆ é™¤å•æ¡è®°å½•"""
        try:
            if self.collection is not None:
                # MongoDBåˆ é™¤
                result = self.collection.delete_one({"record_id": record_id})
                deleted = result.deleted_count > 0
            else:
                # æ–‡ä»¶åˆ é™¤ï¼ˆæ ‡è®°ä¸ºå·²åˆ é™¤ï¼Œå®é™…ä¸Šæ˜¯é‡å†™æ–‡ä»¶ï¼‰
                deleted = self._delete_from_files(record_id)
            
            if deleted:
                self.logger.info(f"âœ… åˆ é™¤è®°å½•: {record_id}")
                
                # ä»Redisç¼“å­˜ä¸­åˆ é™¤
                if hasattr(self, 'redis_client') and self.redis_client is not None:
                    try:
                        self.redis_client.delete(f"analysis_history:{record_id}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Redisåˆ é™¤å¤±è´¥: {e}")
                        
                return True
            else:
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è®°å½•: {record_id}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤è®°å½•å¤±è´¥: {e}")
            return False
    
    def delete_records_by_ids(self, record_ids: List[str]) -> int:
        """æ ¹æ®è®°å½•IDåˆ—è¡¨æ‰¹é‡åˆ é™¤è®°å½•"""
        deleted_count = 0
        
        self.logger.info(f"ğŸ—‘ï¸ å¼€å§‹åˆ é™¤ {len(record_ids)} æ¡è®°å½•: {record_ids[:3]}{'...' if len(record_ids) > 3 else ''}")
        
        try:
            if self.collection is not None:
                # MongoDBæ‰¹é‡åˆ é™¤
                self.logger.info(f"ğŸ“Š ä½¿ç”¨MongoDBåˆ é™¤è®°å½•")
                result = self.collection.delete_many({"record_id": {"$in": record_ids}})
                deleted_count = result.deleted_count
                self.logger.info(f"âœ… MongoDBåˆ é™¤ç»“æœ: {deleted_count}/{len(record_ids)} æ¡è®°å½•")
            else:
                # æ–‡ä»¶æ‰¹é‡åˆ é™¤
                self.logger.info(f"ğŸ“ ä½¿ç”¨æ–‡ä»¶å­˜å‚¨åˆ é™¤è®°å½•")
                for record_id in record_ids:
                    if self._delete_from_files(record_id):
                        deleted_count += 1
                self.logger.info(f"âœ… æ–‡ä»¶åˆ é™¤ç»“æœ: {deleted_count}/{len(record_ids)} æ¡è®°å½•")
            
            # ä»Redisç¼“å­˜ä¸­åˆ é™¤
            if hasattr(self, 'redis_client') and self.redis_client is not None:
                try:
                    keys = [f"analysis_history:{record_id}" for record_id in record_ids]
                    if keys:
                        cache_deleted = self.redis_client.delete(*keys)
                        self.logger.info(f"ğŸ—‚ï¸ Redisç¼“å­˜åˆ é™¤: {cache_deleted} ä¸ªé”®")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Redisæ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
            
            self.logger.info(f"âœ… æ‰¹é‡åˆ é™¤å®Œæˆ: {deleted_count} æ¡è®°å½•")
            return deleted_count
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡åˆ é™¤è®°å½•å¤±è´¥: {e}")
            return deleted_count
    
    def _delete_from_files(self, record_id: str) -> bool:
        """ä»æ–‡ä»¶ä¸­åˆ é™¤è®°å½•ï¼ˆé€šè¿‡é‡å†™æ–‡ä»¶ï¼‰"""
        import os
        from pathlib import Path
        import tempfile
        
        project_root = Path(__file__).parent.parent.parent
        history_dir = project_root / "data" / "analysis_history"
        
        if not history_dir.exists():
            return False
        
        deleted = False
        
        # éå†æ‰€æœ‰å†å²æ–‡ä»¶
        for file_path in history_dir.glob("analysis_*.jsonl"):
            try:
                lines_to_keep = []
                found_record = False
                
                # è¯»å–æ–‡ä»¶ï¼Œè¿‡æ»¤æ‰è¦åˆ é™¤çš„è®°å½•
                with open(file_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            try:
                                record = json.loads(line.strip())
                                if record.get("record_id") == record_id:
                                    found_record = True
                                    deleted = True
                                else:
                                    lines_to_keep.append(line)
                            except json.JSONDecodeError:
                                # ä¿ç•™æ— æ³•è§£æçš„è¡Œ
                                lines_to_keep.append(line)
                
                # å¦‚æœæ‰¾åˆ°äº†è®°å½•ï¼Œé‡å†™æ–‡ä»¶
                if found_record:
                    if lines_to_keep:
                        # æœ‰å…¶ä»–è®°å½•ï¼Œé‡å†™æ–‡ä»¶
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.writelines(lines_to_keep)
                    else:
                        # æ–‡ä»¶ä¸ºç©ºï¼Œåˆ é™¤æ–‡ä»¶
                        os.remove(file_path)
                    break
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}: {e}")
                continue
        
        return deleted

# å…¨å±€å®ä¾‹
_history_manager = None

def get_history_manager():
    """è·å–å†å²è®°å½•ç®¡ç†å™¨å®ä¾‹"""
    global _history_manager
    if _history_manager is None:
        _history_manager = AnalysisHistoryManager()
    return _history_manager
